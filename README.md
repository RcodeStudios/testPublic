### 项目启动
   本项目自2023年8月1日起正式启动，初始阶段已成功接入OpenAI GPT-3.5 API。
<br>
<br>

## 开发日志 
### 2023年8月10日 - 初始调试版本1.0
* 成功接入GPT-4 API，但由于OpenAI的当前限制，GPT-4功能暂未激活。
* 完成基本对话界面的开发，涵盖移动端和PC端，进一步完善用户界面设计。
* 实施用户账户系统，要求新用户注册以使用GPT-3.5服务。
* 计划后续开发包括付费功能在内的多项服务。
<br>

### 2023年8月11日 - 项目开发暂停
* 项目暂时终止开发，原因待后续公告。
<br>

### 2023.12.11 2023年12月11日 - UI优化和功能扩展
* 对用户界面(UI)进行全面优化。
* 成功接入GPT-4 API，关闭新用户注册功能，项目转为公司内部使用。
* 发布release-1.0版本，集成当前领先的AI技术，启动项目解释功能和图像生成功能的开发。
<br>
  
### 2023.12.15 接入代码解释器，DALLE-3图片生成，再次优化UI设计，加入代码块，图片显示区域。
<br>

### 2023年12月15日 - 功能性更新
* 引入代码解释器和DALL-E 3图像生成技术。
* 再次优化用户界面设计，新增代码展示和图像显示区域。
<br>

### 2023年12月18日 - GUI优化和新功能
* 加入文件上传功能。
* 进行显著的图形用户界面(GUI)优化。
<br>

### 2023年12月24日 - 格式支持与功能扩展
* 新增对Markdown格式文本的支持。
* 实现对整个文件夹的上传功能。
* 新增对数学公式格式文本的支持
<br>

### 2023年12月25日 - 功能测试阶段
* 代码解释器和文件上传功能进入灰度测试阶段，暂不开放。
<br>

### 2023年12月28日 - 功能修复与优化
* 完成文件上传功能的修复，支持常见文件类型，包括图像和视频。
* API现可识别图像内容，优化网络框架。
   #### openai图片模型识别局限性（官方文档内容）：
   * 虽然具有视觉功能的 GPT-4 功能强大并且可以在许多情况下使用，但了解该模型的局限性也很重要。以下是我们意识到的一些限制：
   * 医学图像：该模型不适合解释 CT 扫描等专业医学图像，也不应用于提供医疗建议。
   * 非英语：在处理包含非拉丁字母文本（例如日语或韩语）的图像时，模型可能无法获得最佳性能。
   *  小文本：放大图像中的文本以提高可读性，但避免裁剪重要细节。
   *  旋转：模型可能会误解旋转/颠倒的文本或图像。
   *  视觉元素：模型可能难以理解颜色或样式（如实线、虚线或点线）变化的图形或文本。
   *  空间推理：该模型难以完成需要精确空间定位的任务，例如识别国际象棋位置。
   *  准确性：在某些情况下，模型可能会生成不正确的描述或标题。
   *  图像形状：模型难以处理全景和鱼眼图像。
   *  元数据和调整大小：模型不处理原始文件名或元数据，图像在分析之前会调整大小，从而影响其原始尺寸。
   *  计数：可以给出图像中对象的近似计数。
   *  验证码：出于安全原因，我们实施了一个系统来阻止验证码的提交
   #### openaiAPI无法识别图片中的具体内容，我结合了光学字符识别（OCR）技术以尽可能突破模型在这方面的局限性。（等待完成）
   #### openaiAPI对于处理过的图片是没有记忆的，所以在上传图片后，你只能在当前的聊天窗口完成让其识别图片的相关描述，在下一个对话时，openaiAPI将不会记得你上一次上传的图片是什么。
<br>

### 2023年12月30日 - 网络功能增强
* 支持API进行联网访问。
* 与chatGPT不同，本项目接入Google搜索，而非Bing搜索，期待两者性能对比。
* 实现对指定网站URL内容的分析。
* Stable Diffusion AI绘图功能进入灰度测试阶段，因算力限制测试可能较长，同时继续优化GUI表现。
<br>

### 2023年12月30日晚 - 功能调整
* 暂停Python代码解释器的开发。
* 文件上传功能进入灰度测试阶段，史诗级项目的大型更新预期即将到来。
<br>

### 2023年12月31日 - 功能优化
* 完成文件上传功能的优化。
<br>

### 2024年1月1日 - 新项目计划
* 启动项目解释器的开发计划。
<br>

<br>
<br>
<br>


## 服务器及功能运行状态：
* DALL-E 3图像生成：可用
* Stable Diffusion AI绘画：灰度测试阶段
* Python代码解释器：暂时关闭
* 文件上传：可用
* 图像识别：可用
* 项目解释器：开发中
* Beta项目解释器：不可用
* Google搜索（联网）：可用
* 访问指定URL链接的网站：可用
* 文件检索：不可用
* 语音聊天：不可用

<br>
<br>
<br>
<br>

## 已知问题：
   * 用户输入在某些情况下可能被误判为数学公式，导致显示异常。特别是在上传代码（尤其是Python代码）时，此问题更为显著。

<br>
<br>
<br>
<br>


## 项目解释器开发计划流程：
   ### 首先这个项目解释器底层是调用openai的GPT4API的，所有API接口我已经接入完成了。目的是这样的，在客户端上传一个文件夹，这个文件夹就是整个要去解释的项目（前端上传的逻辑我已经写好了）。接着服务器收到客户端上传的文件夹，就可以开始遍历每一个文件了。拿到这个上传的文件夹的同时，还会拿到文件夹中所有文件的相对路径。这个信息应该能帮助API更完美的分析。
   ### 1.第一步：遍历所有文件，计算每一个文件将会占据的输入token数量，在使用的gpt-4-1106-preview模型，它具有128000输入的token和4096输出token。将每一个文件的输入token写成一张表，加上每个文件夹在文件中的路径表丢给模型进行分析，可以让他决定分析文件的顺序（基于文件名和模块），可以让模型返回要分析所有文件的顺序表。

   ### 2.第二步：遍历分析文件的顺序表（这个表是第一步中模型制作的，其实就是让模型生成一个外部函数的参数就行。这个表的制作比较讲究，需要根据模块化以及token上限进行分析，比如有几个文件可能是有依赖的，并且他们的文件输入token都不会超过输入上限，那么就可以累加到一起发送给模型，以减少调用的次数。但是这并不会减少调用的成本，所以将每一个文件单独分析也是可行的，这样或许会更加精确。），在遍历的过程中，将当前要分析的文件发送给gpt-4-1106-preview模型，可以让其为代码的模块添加一些注释，最后提供一个超级完整的该文件的功能总结，如对每一个函数的详细总结。总结完成后，将文件路径名作为key，输出的原文件添加注释后的内容以及对每个函数的总结写入到一张表中作为value。通过这种方式再次写入到一个文件中，加入redis进行缓存。

   ### 3.第三步：在每次遍历的过程中，如果模型认为需要依赖其他已分析好的文件的代码进行更好的分析，可以通过redis获取已经分析好的文件，根据获取的内容进行更深度的分析。

   ### 4.第四步：所有文件分析完成后，我们可以将所有文件总结的函数模块整合到一起，整合的方式包含对应文件的路径。最后根据这些所有总结的集合对整个项目的作用进行分析。

   ### 5.第五步：创建一个Assistants API，将刚刚第二步创建的文件的集合，全部以检索的方式发送给Assistants。接下来就可以开始对话了，这个Assistants线程拥有该项目中所有的检索内容。包括也可以将刚刚第四步分析的结果也加入进来，用户可以通过这种方式实时的获得项目的相关信息。
    
